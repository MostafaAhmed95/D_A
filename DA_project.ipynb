{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DA_project",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MostafaAhmed95/D_A/blob/master/DA_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxnLbblF1Tu8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch. utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRqr-56v184c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "outputId": "6e08b1b8-0eca-4596-d908-2fff61fe5c40"
      },
      "source": [
        "batch_size = 32\n",
        "test_batch_size = 100\n",
        "\n",
        "# Transformations\n",
        "data_transformations = transforms.Compose([\n",
        "                           transforms.Grayscale(num_output_channels=1),\n",
        "                           transforms.Scale((28,28)),\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize((0.1307,), (0.3081,))\n",
        "                       ])\n",
        "\n",
        "# Data Source\n",
        "svhn = datasets.SVHN('../data', download=True, transform=data_transformations)\n",
        "mnist_test = datasets.MNIST('../data', train=False,transform=data_transformations, download=True)\n",
        "\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(svhn,\n",
        "                          batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(mnist_test,\n",
        "                         batch_size=test_batch_size, shuffle=True)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:211: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: ../data/train_32x32.mat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY-qNjVpMNC9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "59bb2059-6802-44d4-a248-039195cf9f34"
      },
      "source": [
        "batch,labels = next(iter(train_loader))\n",
        "image = batch[0]\n",
        "np_image = image.data.numpy()\n",
        "print(np_image.shape)\n",
        "plt.imshow(np_image[0],cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7CKZ+H9E5NG3X7EOXgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqvUmdQVTN3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):#This defines the structure of the NN.def __init__(self):super(Net, self).__init__()\n",
        "        # These are all operations that we are defining.# Unlike keras, this is not the network definition.# This is just initialization of the variables that # we are going to use in the `forward()` function.self.conv1 = nn.Conv2d(1, 10, kernel_size=5)self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        \n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5) \n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5) \n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "#         print(\"1\", x.shape)\n",
        "        #28x28x1\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))#12x12x10\n",
        "#         print(\"2\", x.shape)\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))#4x4x20\n",
        "#         print(\"3\", x.shape)\n",
        "        x = x.view(-1, 320)\n",
        "#         print(\"4\", x.shape)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x)\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model_cnn = Net().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25XCh4XPXLzf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train our model\n",
        "def train( model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                       100. * batch_idx / len(train_loader), loss.item()))\n",
        "            \n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDEL7vzcY-0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test( model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMYb8FmoabTi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5e4baf2c-672f-42da-aebb-ac4a4a80ce74"
      },
      "source": [
        "epochs = 10\n",
        "lr = 0.01\n",
        "momentum = 0.5\n",
        "log_interval = 700\n",
        "\n",
        "model = model_cnn\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n",
        "\n",
        "\n",
        "    torch.save(model.state_dict(), \"sv_inno.pt\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/73257 (0%)]\tLoss: 2.306124\n",
            "Train Epoch: 1 [22400/73257 (31%)]\tLoss: 2.252903\n",
            "Train Epoch: 1 [44800/73257 (61%)]\tLoss: 2.157453\n",
            "Train Epoch: 1 [67200/73257 (92%)]\tLoss: 1.843431\n",
            "\n",
            "Test set: Average loss: 2.2648, Accuracy: 3259/10000 (32.59%)\n",
            "\n",
            "Train Epoch: 2 [0/73257 (0%)]\tLoss: 1.699224\n",
            "Train Epoch: 2 [22400/73257 (31%)]\tLoss: 1.145149\n",
            "Train Epoch: 2 [44800/73257 (61%)]\tLoss: 1.148265\n",
            "Train Epoch: 2 [67200/73257 (92%)]\tLoss: 0.963224\n",
            "\n",
            "Test set: Average loss: 2.1584, Accuracy: 5576/10000 (55.76%)\n",
            "\n",
            "Train Epoch: 3 [0/73257 (0%)]\tLoss: 1.009073\n",
            "Train Epoch: 3 [22400/73257 (31%)]\tLoss: 0.802172\n",
            "Train Epoch: 3 [44800/73257 (61%)]\tLoss: 1.042818\n",
            "Train Epoch: 3 [67200/73257 (92%)]\tLoss: 0.746934\n",
            "\n",
            "Test set: Average loss: 2.2073, Accuracy: 6220/10000 (62.20%)\n",
            "\n",
            "Train Epoch: 4 [0/73257 (0%)]\tLoss: 0.537283\n",
            "Train Epoch: 4 [22400/73257 (31%)]\tLoss: 1.047944\n",
            "Train Epoch: 4 [44800/73257 (61%)]\tLoss: 0.654901\n",
            "Train Epoch: 4 [67200/73257 (92%)]\tLoss: 1.121510\n",
            "\n",
            "Test set: Average loss: 2.2379, Accuracy: 5808/10000 (58.08%)\n",
            "\n",
            "Train Epoch: 5 [0/73257 (0%)]\tLoss: 1.108346\n",
            "Train Epoch: 5 [22400/73257 (31%)]\tLoss: 1.284157\n",
            "Train Epoch: 5 [44800/73257 (61%)]\tLoss: 0.958301\n",
            "Train Epoch: 5 [67200/73257 (92%)]\tLoss: 0.632242\n",
            "\n",
            "Test set: Average loss: 1.9297, Accuracy: 6154/10000 (61.54%)\n",
            "\n",
            "Train Epoch: 6 [0/73257 (0%)]\tLoss: 1.071676\n",
            "Train Epoch: 6 [22400/73257 (31%)]\tLoss: 0.900543\n",
            "Train Epoch: 6 [44800/73257 (61%)]\tLoss: 0.548131\n",
            "Train Epoch: 6 [67200/73257 (92%)]\tLoss: 0.695432\n",
            "\n",
            "Test set: Average loss: 2.1063, Accuracy: 6201/10000 (62.01%)\n",
            "\n",
            "Train Epoch: 7 [0/73257 (0%)]\tLoss: 0.777378\n",
            "Train Epoch: 7 [22400/73257 (31%)]\tLoss: 0.615065\n",
            "Train Epoch: 7 [44800/73257 (61%)]\tLoss: 0.846899\n",
            "Train Epoch: 7 [67200/73257 (92%)]\tLoss: 0.562487\n",
            "\n",
            "Test set: Average loss: 2.2315, Accuracy: 5823/10000 (58.23%)\n",
            "\n",
            "Train Epoch: 8 [0/73257 (0%)]\tLoss: 0.463229\n",
            "Train Epoch: 8 [22400/73257 (31%)]\tLoss: 0.708185\n",
            "Train Epoch: 8 [44800/73257 (61%)]\tLoss: 0.726969\n",
            "Train Epoch: 8 [67200/73257 (92%)]\tLoss: 0.740045\n",
            "\n",
            "Test set: Average loss: 2.1423, Accuracy: 5652/10000 (56.52%)\n",
            "\n",
            "Train Epoch: 9 [0/73257 (0%)]\tLoss: 0.790855\n",
            "Train Epoch: 9 [22400/73257 (31%)]\tLoss: 0.712206\n",
            "Train Epoch: 9 [44800/73257 (61%)]\tLoss: 0.504986\n",
            "Train Epoch: 9 [67200/73257 (92%)]\tLoss: 0.795173\n",
            "\n",
            "Test set: Average loss: 2.0799, Accuracy: 5892/10000 (58.92%)\n",
            "\n",
            "Train Epoch: 10 [0/73257 (0%)]\tLoss: 0.619951\n",
            "Train Epoch: 10 [22400/73257 (31%)]\tLoss: 0.634546\n",
            "Train Epoch: 10 [44800/73257 (61%)]\tLoss: 0.814057\n",
            "Train Epoch: 10 [67200/73257 (92%)]\tLoss: 0.856529\n",
            "\n",
            "Test set: Average loss: 2.4935, Accuracy: 6043/10000 (60.43%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}